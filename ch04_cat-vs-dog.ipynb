{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from glob import glob\n",
    "import cv2, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# !kaggle datasets download -d tongpython/cat-and-dog    # 下载数据集\n",
    "# !mkdir content\n",
    "# !unzip cat-and-dog.zip -d content\n",
    "\n",
    "\n",
    "class cats_dogs_dataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        cats = glob(folder + \"/cats/*.jpg\")\n",
    "        dogs = glob(folder + \"/dogs/*.jpg\")\n",
    "        self.fpaths = cats + dogs\n",
    "        from random import shuffle, seed\n",
    "\n",
    "        seed(10)\n",
    "        shuffle(self.fpaths)\n",
    "        self.targets = [\n",
    "            fpath.split(\"/\")[-1].startswith(\"dog\") for fpath in self.fpaths\n",
    "        ]  # dog=1 & cat=0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fpaths)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        f = self.fpaths[ix]\n",
    "        target = self.targets[ix]\n",
    "        im = cv2.imread(f)[:, :, ::-1]\n",
    "        im = cv2.resize(im, (224, 224))\n",
    "        return torch.tensor(im / 255).permute(2, 0, 1).to(device).float(), torch.tensor(\n",
    "            [target]\n",
    "        ).float().to(device)\n",
    "\n",
    "\n",
    "def conv_layer(ni, no, kernel_size, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, no, kernel_size, stride),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(no),\n",
    "        nn.MaxPool2d(2),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        conv_layer(3, 64, 3),\n",
    "        conv_layer(64, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "\n",
    "def train_batch(x, y, model, optimizer, loss_fn):\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return batch_loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(x, y, model):\n",
    "    prediction = model(x)\n",
    "    is_correct = (prediction > 0.5) == y\n",
    "    return is_correct.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_data(train_data_dir, test_data_dir):\n",
    "\n",
    "    train = cats_dogs_dataset(train_data_dir)\n",
    "    trn_dl = DataLoader(train, batch_size=32, shuffle=True, drop_last=True)\n",
    "    val = cats_dogs_dataset(test_data_dir)\n",
    "    val_dl = DataLoader(val, batch_size=32, shuffle=True, drop_last=True)\n",
    "    return trn_dl, val_dl\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss(x, y, model, loss_fn):\n",
    "    prediction = model(x)\n",
    "    # global loss_fn\n",
    "    val_loss = loss_fn(prediction, y)\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_data_dir = \"./content/training_set/training_set\"\n",
    "    test_data_dir = \"./content/test_set/test_set\"\n",
    "\n",
    "    trn_dl, val_dl = get_data(train_data_dir, test_data_dir)\n",
    "    model, loss_fn, optimizer = get_model()\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "\n",
    "        train_epoch_losses, train_epoch_accuracies = [], []\n",
    "        val_epoch_accuracies = []\n",
    "        val_epoch_losses = []\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for ix, batch in enumerate(trn_dl):\n",
    "            print(f\"\\ttraining: {ix}\")\n",
    "            x, y = batch\n",
    "\n",
    "            batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "            train_epoch_losses.append(batch_loss)\n",
    "\n",
    "            is_correct = accuracy(x, y, model)\n",
    "            train_epoch_accuracies.extend(is_correct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        for ix, batch in enumerate(val_dl):\n",
    "            print(f\"\\tevaluating: {ix}\")\n",
    "            x, y = batch\n",
    "            val_is_correct = accuracy(x, y, model)\n",
    "            val_epoch_accuracies.extend(val_is_correct)\n",
    "            validation_loss = val_loss(x, y, model, loss_fn)\n",
    "            val_epoch_losses.append(validation_loss)\n",
    "\n",
    "        train_epoch_loss = np.mean(train_epoch_losses)\n",
    "        train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "        val_epoch_loss = np.mean(val_epoch_losses)\n",
    "        val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
    "\n",
    "        train_losses.append(train_epoch_loss)\n",
    "        train_accuracies.append(train_epoch_accuracy)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss: {train_epoch_loss:.4f}, Train Accuracy: {train_epoch_accuracy:.4f}\"\n",
    "        )\n",
    "        print(f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_accuracy:.4f}\")\n",
    "\n",
    "    return train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "\ttraining: 0\n",
      "\ttraining: 1\n",
      "\ttraining: 2\n",
      "\ttraining: 3\n",
      "\ttraining: 4\n",
      "\ttraining: 5\n",
      "\ttraining: 6\n",
      "\ttraining: 7\n",
      "\ttraining: 8\n",
      "\ttraining: 9\n",
      "\ttraining: 10\n",
      "\ttraining: 11\n",
      "\ttraining: 12\n",
      "\ttraining: 13\n",
      "\ttraining: 14\n",
      "\ttraining: 15\n",
      "\ttraining: 16\n",
      "\ttraining: 17\n",
      "\ttraining: 18\n",
      "\ttraining: 19\n",
      "\ttraining: 20\n",
      "\ttraining: 21\n",
      "\ttraining: 22\n",
      "\ttraining: 23\n",
      "\ttraining: 24\n",
      "\ttraining: 25\n",
      "\ttraining: 26\n",
      "\ttraining: 27\n",
      "\ttraining: 28\n",
      "\ttraining: 29\n",
      "\ttraining: 30\n",
      "\ttraining: 31\n",
      "\ttraining: 32\n",
      "\ttraining: 33\n",
      "\ttraining: 34\n",
      "\ttraining: 35\n",
      "\ttraining: 36\n",
      "\ttraining: 37\n",
      "\ttraining: 38\n",
      "\ttraining: 39\n",
      "\ttraining: 40\n",
      "\ttraining: 41\n",
      "\ttraining: 42\n",
      "\ttraining: 43\n",
      "\ttraining: 44\n",
      "\ttraining: 45\n",
      "\ttraining: 46\n",
      "\ttraining: 47\n",
      "\ttraining: 48\n",
      "\ttraining: 49\n",
      "\ttraining: 50\n",
      "\ttraining: 51\n",
      "\ttraining: 52\n",
      "\ttraining: 53\n",
      "\ttraining: 54\n",
      "\ttraining: 55\n",
      "\ttraining: 56\n",
      "\ttraining: 57\n",
      "\ttraining: 58\n",
      "\ttraining: 59\n",
      "\ttraining: 60\n",
      "\ttraining: 61\n",
      "\ttraining: 62\n",
      "\ttraining: 63\n",
      "\ttraining: 64\n",
      "\ttraining: 65\n",
      "\ttraining: 66\n",
      "\ttraining: 67\n",
      "\ttraining: 68\n",
      "\ttraining: 69\n",
      "\ttraining: 70\n",
      "\ttraining: 71\n",
      "\ttraining: 72\n",
      "\ttraining: 73\n",
      "\ttraining: 74\n",
      "\ttraining: 75\n",
      "\ttraining: 76\n",
      "\ttraining: 77\n",
      "\ttraining: 78\n",
      "\ttraining: 79\n",
      "\ttraining: 80\n",
      "\ttraining: 81\n",
      "\ttraining: 82\n",
      "\ttraining: 83\n",
      "\ttraining: 84\n",
      "\ttraining: 85\n",
      "\ttraining: 86\n",
      "\ttraining: 87\n",
      "\ttraining: 88\n",
      "\ttraining: 89\n",
      "\ttraining: 90\n",
      "\ttraining: 91\n",
      "\ttraining: 92\n",
      "\ttraining: 93\n",
      "\ttraining: 94\n",
      "\ttraining: 95\n",
      "\ttraining: 96\n",
      "\ttraining: 97\n",
      "\ttraining: 98\n",
      "\ttraining: 99\n",
      "\ttraining: 100\n",
      "\ttraining: 101\n",
      "\ttraining: 102\n",
      "\ttraining: 103\n",
      "\ttraining: 104\n",
      "\ttraining: 105\n",
      "\ttraining: 106\n",
      "\ttraining: 107\n",
      "\ttraining: 108\n",
      "\ttraining: 109\n",
      "\ttraining: 110\n",
      "\ttraining: 111\n",
      "\ttraining: 112\n",
      "\ttraining: 113\n",
      "\ttraining: 114\n",
      "\ttraining: 115\n",
      "\ttraining: 116\n",
      "\ttraining: 117\n",
      "\ttraining: 118\n",
      "\ttraining: 119\n",
      "\ttraining: 120\n",
      "\ttraining: 121\n",
      "\ttraining: 122\n",
      "\ttraining: 123\n",
      "\ttraining: 124\n",
      "\ttraining: 125\n",
      "\ttraining: 126\n",
      "\ttraining: 127\n",
      "\ttraining: 128\n",
      "\ttraining: 129\n",
      "\ttraining: 130\n",
      "\ttraining: 131\n",
      "\ttraining: 132\n",
      "\ttraining: 133\n",
      "\ttraining: 134\n",
      "\ttraining: 135\n",
      "\ttraining: 136\n",
      "\ttraining: 137\n",
      "\ttraining: 138\n",
      "\ttraining: 139\n",
      "\ttraining: 140\n",
      "\ttraining: 141\n",
      "\ttraining: 142\n",
      "\ttraining: 143\n",
      "\ttraining: 144\n",
      "\ttraining: 145\n",
      "\ttraining: 146\n",
      "\ttraining: 147\n",
      "\ttraining: 148\n",
      "\ttraining: 149\n",
      "\ttraining: 150\n",
      "\ttraining: 151\n",
      "\ttraining: 152\n",
      "\ttraining: 153\n",
      "\ttraining: 154\n",
      "\ttraining: 155\n",
      "\ttraining: 156\n",
      "\ttraining: 157\n",
      "\ttraining: 158\n",
      "\ttraining: 159\n",
      "\ttraining: 160\n",
      "\ttraining: 161\n",
      "\ttraining: 162\n",
      "\ttraining: 163\n",
      "\ttraining: 164\n",
      "\ttraining: 165\n",
      "\ttraining: 166\n",
      "\ttraining: 167\n",
      "\ttraining: 168\n",
      "\ttraining: 169\n",
      "\ttraining: 170\n",
      "\ttraining: 171\n",
      "\ttraining: 172\n",
      "\ttraining: 173\n",
      "\ttraining: 174\n",
      "\ttraining: 175\n",
      "\ttraining: 176\n",
      "\ttraining: 177\n",
      "\ttraining: 178\n",
      "\ttraining: 179\n",
      "\ttraining: 180\n",
      "\ttraining: 181\n",
      "\ttraining: 182\n",
      "\ttraining: 183\n",
      "\ttraining: 184\n",
      "\ttraining: 185\n",
      "\ttraining: 186\n",
      "\ttraining: 187\n",
      "\ttraining: 188\n",
      "\ttraining: 189\n",
      "\ttraining: 190\n",
      "\ttraining: 191\n",
      "\ttraining: 192\n",
      "\ttraining: 193\n",
      "\ttraining: 194\n",
      "\ttraining: 195\n",
      "\ttraining: 196\n",
      "\ttraining: 197\n",
      "\ttraining: 198\n",
      "\ttraining: 199\n",
      "\ttraining: 200\n",
      "\ttraining: 201\n",
      "\ttraining: 202\n",
      "\ttraining: 203\n",
      "\ttraining: 204\n",
      "\ttraining: 205\n",
      "\ttraining: 206\n",
      "\ttraining: 207\n",
      "\ttraining: 208\n",
      "\ttraining: 209\n",
      "\ttraining: 210\n",
      "\ttraining: 211\n",
      "\ttraining: 212\n",
      "\ttraining: 213\n",
      "\ttraining: 214\n",
      "\ttraining: 215\n",
      "\ttraining: 216\n",
      "\ttraining: 217\n",
      "\ttraining: 218\n",
      "\ttraining: 219\n",
      "\ttraining: 220\n",
      "\ttraining: 221\n",
      "\ttraining: 222\n",
      "\ttraining: 223\n",
      "\ttraining: 224\n",
      "\ttraining: 225\n",
      "\ttraining: 226\n",
      "\ttraining: 227\n",
      "\ttraining: 228\n",
      "\ttraining: 229\n",
      "\ttraining: 230\n",
      "\ttraining: 231\n",
      "\ttraining: 232\n",
      "\ttraining: 233\n",
      "\ttraining: 234\n",
      "\ttraining: 235\n",
      "\ttraining: 236\n",
      "\ttraining: 237\n",
      "\ttraining: 238\n",
      "\ttraining: 239\n",
      "\ttraining: 240\n",
      "\ttraining: 241\n",
      "\ttraining: 242\n",
      "\ttraining: 243\n",
      "\ttraining: 244\n",
      "\ttraining: 245\n",
      "\ttraining: 246\n",
      "\ttraining: 247\n",
      "\ttraining: 248\n",
      "\ttraining: 249\n",
      "\tevaluating: 0\n",
      "\tevaluating: 1\n",
      "\tevaluating: 2\n",
      "\tevaluating: 3\n",
      "\tevaluating: 4\n",
      "\tevaluating: 5\n",
      "\tevaluating: 6\n",
      "\tevaluating: 7\n",
      "\tevaluating: 8\n",
      "\tevaluating: 9\n",
      "\tevaluating: 10\n",
      "\tevaluating: 11\n",
      "\tevaluating: 12\n",
      "\tevaluating: 13\n",
      "\tevaluating: 14\n",
      "\tevaluating: 15\n",
      "\tevaluating: 16\n",
      "\tevaluating: 17\n",
      "\tevaluating: 18\n",
      "\tevaluating: 19\n",
      "\tevaluating: 20\n",
      "\tevaluating: 21\n",
      "\tevaluating: 22\n",
      "\tevaluating: 23\n",
      "\tevaluating: 24\n",
      "\tevaluating: 25\n",
      "\tevaluating: 26\n",
      "\tevaluating: 27\n",
      "\tevaluating: 28\n",
      "\tevaluating: 29\n",
      "\tevaluating: 30\n",
      "\tevaluating: 31\n",
      "\tevaluating: 32\n",
      "\tevaluating: 33\n",
      "\tevaluating: 34\n",
      "\tevaluating: 35\n",
      "\tevaluating: 36\n",
      "\tevaluating: 37\n",
      "\tevaluating: 38\n",
      "\tevaluating: 39\n",
      "\tevaluating: 40\n",
      "\tevaluating: 41\n",
      "\tevaluating: 42\n",
      "\tevaluating: 43\n",
      "\tevaluating: 44\n",
      "\tevaluating: 45\n",
      "\tevaluating: 46\n",
      "\tevaluating: 47\n",
      "\tevaluating: 48\n",
      "\tevaluating: 49\n",
      "\tevaluating: 50\n",
      "\tevaluating: 51\n",
      "\tevaluating: 52\n",
      "\tevaluating: 53\n",
      "\tevaluating: 54\n",
      "\tevaluating: 55\n",
      "\tevaluating: 56\n",
      "\tevaluating: 57\n",
      "\tevaluating: 58\n",
      "\tevaluating: 59\n",
      "\tevaluating: 60\n",
      "\tevaluating: 61\n",
      "\tevaluating: 62\n",
      "Train Loss: 0.6873, Train Accuracy: 0.6765\n",
      "Val Loss: 0.5405, Val Accuracy: 0.7371\n",
      "epoch: 1\n",
      "\ttraining: 0\n",
      "\ttraining: 1\n",
      "\ttraining: 2\n",
      "\ttraining: 3\n",
      "\ttraining: 4\n",
      "\ttraining: 5\n",
      "\ttraining: 6\n",
      "\ttraining: 7\n",
      "\ttraining: 8\n",
      "\ttraining: 9\n",
      "\ttraining: 10\n",
      "\ttraining: 11\n",
      "\ttraining: 12\n",
      "\ttraining: 13\n",
      "\ttraining: 14\n",
      "\ttraining: 15\n",
      "\ttraining: 16\n",
      "\ttraining: 17\n",
      "\ttraining: 18\n",
      "\ttraining: 19\n",
      "\ttraining: 20\n",
      "\ttraining: 21\n",
      "\ttraining: 22\n",
      "\ttraining: 23\n",
      "\ttraining: 24\n",
      "\ttraining: 25\n",
      "\ttraining: 26\n",
      "\ttraining: 27\n",
      "\ttraining: 28\n",
      "\ttraining: 29\n",
      "\ttraining: 30\n",
      "\ttraining: 31\n",
      "\ttraining: 32\n",
      "\ttraining: 33\n",
      "\ttraining: 34\n",
      "\ttraining: 35\n",
      "\ttraining: 36\n",
      "\ttraining: 37\n",
      "\ttraining: 38\n",
      "\ttraining: 39\n",
      "\ttraining: 40\n",
      "\ttraining: 41\n",
      "\ttraining: 42\n",
      "\ttraining: 43\n",
      "\ttraining: 44\n",
      "\ttraining: 45\n",
      "\ttraining: 46\n",
      "\ttraining: 47\n",
      "\ttraining: 48\n",
      "\ttraining: 49\n",
      "\ttraining: 50\n",
      "\ttraining: 51\n",
      "\ttraining: 52\n",
      "\ttraining: 53\n",
      "\ttraining: 54\n",
      "\ttraining: 55\n",
      "\ttraining: 56\n",
      "\ttraining: 57\n",
      "\ttraining: 58\n",
      "\ttraining: 59\n",
      "\ttraining: 60\n",
      "\ttraining: 61\n",
      "\ttraining: 62\n",
      "\ttraining: 63\n",
      "\ttraining: 64\n",
      "\ttraining: 65\n",
      "\ttraining: 66\n",
      "\ttraining: 67\n",
      "\ttraining: 68\n",
      "\ttraining: 69\n",
      "\ttraining: 70\n",
      "\ttraining: 71\n",
      "\ttraining: 72\n",
      "\ttraining: 73\n",
      "\ttraining: 74\n",
      "\ttraining: 75\n",
      "\ttraining: 76\n",
      "\ttraining: 77\n",
      "\ttraining: 78\n",
      "\ttraining: 79\n",
      "\ttraining: 80\n",
      "\ttraining: 81\n",
      "\ttraining: 82\n",
      "\ttraining: 83\n",
      "\ttraining: 84\n",
      "\ttraining: 85\n",
      "\ttraining: 86\n",
      "\ttraining: 87\n",
      "\ttraining: 88\n",
      "\ttraining: 89\n",
      "\ttraining: 90\n",
      "\ttraining: 91\n",
      "\ttraining: 92\n",
      "\ttraining: 93\n",
      "\ttraining: 94\n",
      "\ttraining: 95\n"
     ]
    }
   ],
   "source": [
    "train_accuracies, val_accuracies = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmticker\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, \u001b[43mtrain_accuracies\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_accuracies, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mgca()\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_major_locator(mticker\u001b[38;5;241m.\u001b[39mMultipleLocator(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = np.arange(5)+1\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.title('Training and validation accuracy with 4K data points used for training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim(0.8,1)\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
