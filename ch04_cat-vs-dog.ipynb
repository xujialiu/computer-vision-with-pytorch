{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%!\n",
    "kaggle datasets download -d tongpython/cat-and-dog\n",
    "unzip cat-and-dog.zip -d content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from glob import glob\n",
    "import cv2, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        cats = glob(folder + \"/cats/*.jpg\")\n",
    "        dogs = glob(folder + \"/dogs/*.jpg\")\n",
    "        self.fpaths = cats + dogs\n",
    "        from random import shuffle, seed\n",
    "\n",
    "        seed(10)\n",
    "        shuffle(self.fpaths)\n",
    "        self.targets = [\n",
    "            fpath.split(\"/\")[-1].startswith(\"dog\") for fpath in self.fpaths\n",
    "        ]  # dog=1 & cat=0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fpaths)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        f = self.fpaths[ix]\n",
    "        target = self.targets[ix]\n",
    "        im = cv2.imread(f)[:, :, ::-1]\n",
    "        im = cv2.resize(im, (224, 224))\n",
    "        return torch.tensor(im / 255).permute(2, 0, 1).to(device).float(), torch.tensor(\n",
    "            [target]\n",
    "        ).float().to(device)\n",
    "\n",
    "\n",
    "def conv_layer(ni, no, kernel_size, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, no, kernel_size, stride),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(no),\n",
    "        nn.MaxPool2d(2),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        conv_layer(3, 64, 3),\n",
    "        conv_layer(64, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        conv_layer(512, 512, 3),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "\n",
    "def train_batch(x, y, model, optimizer, loss_fn):\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return batch_loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(x, y, model):\n",
    "    prediction = model(x)\n",
    "    is_correct = (prediction > 0.5) == y\n",
    "    return is_correct.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_data(train_data_dir, test_data_dir):\n",
    "\n",
    "    train = CatsDogsDataset(train_data_dir)\n",
    "    trn_dl = DataLoader(train, batch_size=32, shuffle=True, drop_last=True)\n",
    "    val = CatsDogsDataset(test_data_dir)\n",
    "    val_dl = DataLoader(val, batch_size=32, shuffle=True, drop_last=True)\n",
    "    return trn_dl, val_dl\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss(x, y, model, loss_fn):\n",
    "    prediction = model(x)\n",
    "    val_loss = loss_fn(prediction, y)\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_data_dir = \"./content/training_set/training_set\"\n",
    "    test_data_dir = \"./content/test_set/test_set\"\n",
    "\n",
    "    trn_dl, val_dl = get_data(train_data_dir, test_data_dir)\n",
    "    model, loss_fn, optimizer = get_model()\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"epoch: {epoch+1}\")\n",
    "\n",
    "        train_epoch_losses, train_epoch_accuracies = [], []\n",
    "        val_epoch_accuracies = []\n",
    "        val_epoch_losses = []\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        num_batch = len(trn_dl)\n",
    "        for idx, batch in enumerate(trn_dl, 1):\n",
    "            print(f\"\\ttraining: {idx}/{num_batch}\")\n",
    "            x, y = batch\n",
    "\n",
    "            batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "            train_epoch_losses.append(batch_loss)\n",
    "\n",
    "            is_correct = accuracy(x, y, model)\n",
    "            train_epoch_accuracies.extend(is_correct)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        for idx, batch in enumerate(val_dl, 1):\n",
    "            print(f\"\\tevaluating: {idx}/{num_batch}\")\n",
    "            \n",
    "            x, y = batch\n",
    "            val_is_correct = accuracy(x, y, model)\n",
    "            val_epoch_accuracies.extend(val_is_correct)\n",
    "            \n",
    "            validation_loss = val_loss(x, y, model, loss_fn)\n",
    "            val_epoch_losses.append(validation_loss)\n",
    "\n",
    "        train_epoch_loss = np.mean(train_epoch_losses)\n",
    "        train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "        val_epoch_loss = np.mean(val_epoch_losses)\n",
    "        val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
    "\n",
    "        train_losses.append(train_epoch_loss)\n",
    "        train_accuracies.append(train_epoch_accuracy)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss: {train_epoch_loss:.4f}, Train Accuracy: {train_epoch_accuracy:.4f}\"\n",
    "        )\n",
    "        print(f\"Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_accuracy:.4f}\")\n",
    "\n",
    "    return train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, val_accuracies = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = np.arange(5)+1\n",
    "plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.title('Training and validation accuracy with 4K data points used for training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8,1)\n",
    "plt.gca().set_yticklabels([f'{x*100:.0f}%' for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
